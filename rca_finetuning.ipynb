{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d244ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# --- PDF → text utility (PyMuPDF)\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    fitz = None  # Will error later if preprocess is used\n",
    "\n",
    "# --- Deduplication\n",
    "import text_dedup.minhash\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# --- Hugging Face & PEFT\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39dbc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess config\n",
    "pdf_dir = './pdf_swat'\n",
    "out_dir = './data'\n",
    "min_tokens = 128\n",
    "max_tokens = 2048\n",
    "overlap = 256\n",
    "\n",
    "# train config\n",
    "dataset_path = './data/train.jsonl'\n",
    "output_dir = './checkpoints'\n",
    "num_epochs = 3\n",
    "per_device_train_batch = 2\n",
    "grad_accum = 4\n",
    "lr = 2e-4\n",
    "warmup_steps = 100\n",
    "lora_r = 64\n",
    "lora_alpha = 128\n",
    "lora_dropout = 0.05\n",
    "max_seq_len = 2048\n",
    "fp16 = True  # 또는 False\n",
    "\n",
    "# evaluate config\n",
    "adapter_dir = './checkpoints/adapter'\n",
    "eval_dataset_path = './data/eval.json'\n",
    "eval_questions = './data/questions.csv'  # CSV with columns 'question','answer_regex'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc4555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Final chunks: 121\n",
      "[✓] Saved data/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# SECTION 제거용 정규표현식\n",
    "SECTION_PATTERNS = [\n",
    "    re.compile(r\"^references$\", re.I),\n",
    "    re.compile(r\"^bibliography$\", re.I),\n",
    "    re.compile(r\"^acknowledg(e)?ments?$\", re.I),\n",
    "]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove references/acknowledgment sections & excessive blank lines.\"\"\"\n",
    "    lines = [l.strip() for l in text.splitlines()]\n",
    "    cleaned: List[str] = []\n",
    "    skip = False\n",
    "    for ln in lines:\n",
    "        if any(p.match(ln.lower()) for p in SECTION_PATTERNS):\n",
    "            skip = True\n",
    "        if not skip and ln:\n",
    "            cleaned.append(ln)\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "def chunk_text(text: str, tokenizer, max_tokens: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Slice long text into overlapping chunks by token count.\"\"\"\n",
    "    tokens = tokenizer(text)[\"input_ids\"]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk_tokens = tokens[i: i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        if len(chunk_tokens) >= 32:\n",
    "            chunks.append(chunk_text)\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "# 경로 설정\n",
    "swat_txt_path = Path(\"swat.txt\")  # 각 줄이 하나의 논문 텍스트\n",
    "out_dir = Path(\"data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 토크나이저 불러오기\n",
    "with open(\"token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=token)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "max_tokens = 512\n",
    "overlap = 64\n",
    "min_tokens = 64\n",
    "\n",
    "# 처리 시작\n",
    "raw_records = []\n",
    "with swat_txt_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        raw = line.strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        cleaned = clean_text(raw)\n",
    "        raw_records.append({\"doc_id\": f\"swat_{i:03}\", \"text\": cleaned})\n",
    "\n",
    "# 청크 생성 및 필터링\n",
    "all_chunks = []\n",
    "for rec in raw_records:\n",
    "    chunks = chunk_text(rec[\"text\"], tokenizer, max_tokens, overlap)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\"text\": chunk, \"source\": f\"{rec['doc_id']}§{idx}\"})\n",
    "\n",
    "def token_len(example):\n",
    "    return len(tokenizer(example[\"text\"])[\"input_ids\"])\n",
    "\n",
    "all_chunks = [c for c in all_chunks if token_len(c) >= min_tokens]\n",
    "print(f\"[i] Final chunks: {len(all_chunks)}\")\n",
    "\n",
    "# JSONL로 저장\n",
    "jsonl_path = out_dir / \"train.jsonl\"\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in all_chunks:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"[✓] Saved {jsonl_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff7320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Extracted pdf_swat/swat (1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Extracted pdf_swat/1911.04831v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Extracted pdf_swat/swat.pdf\n",
      "[+] Extracted pdf_swat/16523-Article Text-20017-1-2-20210518 (4).pdf\n",
      "[i] Final chunks: 25\n",
      "[✓] Saved data/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract raw text from a single PDF using pdfplumber.\"\"\"\n",
    "    text_chunks = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text_chunks.append(page_text)\n",
    "    return \"\\n\".join(text_chunks)\n",
    "\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     text_chunks = []\n",
    "#     for page in doc:\n",
    "#         blocks = page.get_text(\"blocks\")\n",
    "#         page_text = \"\\n\".join([b[4] for b in blocks])\n",
    "#         text_chunks.append(page_text)\n",
    "#     return \"\\n\".join(text_chunks)\n",
    "\n",
    "\n",
    "SECTION_PATTERNS = [\n",
    "    re.compile(r\"^references$\", re.I),\n",
    "    re.compile(r\"^bibliography$\", re.I),\n",
    "    re.compile(r\"^acknowledg(e)?ments?$\", re.I),\n",
    "]\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove references/acknowledgment sections & excessive blank lines.\"\"\"\n",
    "    lines = [l.strip() for l in text.splitlines()]\n",
    "    cleaned: List[str] = []\n",
    "    skip = False\n",
    "    for ln in lines:\n",
    "        if any(p.match(ln.lower()) for p in SECTION_PATTERNS):\n",
    "            skip = True\n",
    "        if not skip and ln:\n",
    "            cleaned.append(ln)\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "\n",
    "def chunk_text(text: str, tokenizer, max_tokens: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Slice long text into overlapping chunks by token count.\"\"\"\n",
    "    tokens = tokenizer(text)[\"input_ids\"]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk_tokens = tokens[i : i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        if len(chunk_tokens) >= 32:  # minimal meaningful length\n",
    "            chunks.append(chunk_text)\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "pdf_dir = Path(pdf_dir)\n",
    "out_dir = Path(out_dir)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(\"token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=token)\n",
    "\n",
    "raw_records = []\n",
    "for pdf_path in pdf_dir.rglob(\"*.pdf\"):\n",
    "    raw = extract_text_from_pdf(pdf_path)\n",
    "    cleaned = clean_text(raw)\n",
    "    raw_records.append({\"doc_id\": pdf_path.stem, \"text\": cleaned})\n",
    "    print(f\"[+] Extracted {pdf_path}\")\n",
    "\n",
    "# Deduplicate\n",
    "# threshold = 0.88\n",
    "# num_perm = 128\n",
    "\n",
    "# texts = [r[\"text\"] for r in raw_records]\n",
    "\n",
    "# minhashes = []\n",
    "# for text in texts:\n",
    "#     m = MinHash(num_perm=num_perm)\n",
    "#     for word in text.split():\n",
    "#         m.update(word.encode('utf8'))\n",
    "#     minhashes.append(m)\n",
    "\n",
    "# lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "# unique_indices = []\n",
    "# seen = set()\n",
    "\n",
    "# for i, m in enumerate(minhashes):\n",
    "#     duplicates = lsh.query(m)\n",
    "#     if not duplicates:\n",
    "#         lsh.insert(f\"m{i}\", m)\n",
    "#         unique_indices.append(i)\n",
    "\n",
    "# unique_records = [raw_records[i] for i in unique_indices]\n",
    "# print(f\"[i] Deduplicated: {len(texts)} → {len(unique_records)} docs\")\n",
    "\n",
    "# Chunking\n",
    "all_chunks = []\n",
    "for rec in raw_records:\n",
    "    chunks = chunk_text(rec[\"text\"], tokenizer, max_tokens, overlap)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\"text\": chunk, \"source\": f\"{rec['doc_id']}§{idx}\"})\n",
    "\n",
    "# Filter by min_tokens\n",
    "min_toks = min_tokens\n",
    "def token_len(example):\n",
    "    return len(tokenizer(example[\"text\"])[\"input_ids\"])\n",
    "\n",
    "all_chunks = [c for c in all_chunks if token_len(c) >= min_toks]\n",
    "print(f\"[i] Final chunks: {len(all_chunks)}\")\n",
    "\n",
    "# Write JSONL\n",
    "jsonl_path = out_dir / \"train.jsonl\"\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in all_chunks:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"[✓] Saved {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60391afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from datasets import load_dataset, Dataset\n",
    "# import re\n",
    "# import unicodedata\n",
    "# from unidecode import unidecode\n",
    "\n",
    "# def remove_weird_unicode(text):\n",
    "#     text = ''.join(ch for ch in text if unicodedata.category(ch)[0] not in ['C'])\n",
    "#     text = re.sub(r'[\\u2000-\\u200f\\u2028-\\u202f\\u205f\\u2060-\\u206f\\ufeff]', ' ', text)\n",
    "#     text = re.sub(r'[\\u2190-\\u21ff\\u2300-\\u23ff\\u2500-\\u257f\\u2700-\\u27bf\\ue000-\\uf8ff\\U0001F000-\\U0001FAFF]', '', text)\n",
    "#     return text\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = unicodedata.normalize('NFKC', text)\n",
    "#     text = re.sub(r'(?<=[가-힣])\\s+(?=[가-힣])', '', text)\n",
    "#     text = re.sub(r'(?<=[a-zA-Z])\\s+(?=[a-zA-Z])', '', text)\n",
    "#     text = remove_weird_unicode(text)\n",
    "#     text = re.sub(r'[�]', '', text)\n",
    "#     text = unidecode(text)\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     return text.strip()\n",
    "\n",
    "# dataset_path = Path(dataset_path)\n",
    "# if not dataset_path.exists():\n",
    "#     raise FileNotFoundError(dataset_path)\n",
    "\n",
    "# dataset = load_dataset(\"json\", data_files=str(dataset_path), split=\"train\")\n",
    "\n",
    "# dataset = dataset.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
    "\n",
    "# dataset.to_json(\"train_cleaned.jsonl\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7122d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e303decbbb43508fcfcdd01e8e2cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda56e255e5f454bbd132402adc20d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9e5bc3776943ef87b7cb5abf748930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1743322/3783776678.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.376400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.043500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6823dba8-730323ba6353bccb4e3f286b;d3e9f61d-fabd-4c6a-a639-aa3a0b03ca9c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6823dbbb-620021a039ea44792a9d3b22;71a67887-dd5e-4d3f-baec-a87b60bbab80)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6823dbca-2b964f9e573c570466572bef;d3806a01-870b-4c1c-9865-c956b6219e36)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6823dbcc-71b8d8b22ae0091f5bb63c38;09aa3913-4f1d-4ae9-8e23-d9c852bc0e83)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Training complete - adapter+tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path(dataset_path)\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(dataset_path)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=str(dataset_path), split=\"train\")\n",
    "\n",
    "with open(\"token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "# Tokenizer & model\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4-bit QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_auth_token=token, device_map=\"auto\")\n",
    "\n",
    "# PEFT config\n",
    "lora_cfg = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize dataset lazily\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_seq_len)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\", \"source\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=not fp16,\n",
    "    fp16=fp16,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"[✓] Training complete - adapter+tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca29d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='nltk')\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e45452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Domain PPL ≈ 8.24\n",
      "[Answer]: CVD utilizes chemical reactions on substrates with gaseous precursors, while PVD relies on physical vaporization/condensation without chemical changes.\n",
      "[0] BLEU: 8.614911585158347e-232, ROUGE-L: 0.07594936708860761\n",
      "[Answer]: 300-400°C for substrate-specific deposition through surface-limited reactions.\n",
      "[1] BLEU: 7.884916681118857e-232, ROUGE-L: 0.05797101449275362\n",
      "[Answer]: 0.5-2 Torr achieves >95% conformality in high-aspect-ratio structures.\n",
      "[2] BLEU: 6.441148769597431e-232, ROUGE-L: 0.02631578947368421\n",
      "[Answer]: Growth rate increases logarithmically from 0.5 to 3 μm/min as H₂ flow rises from 10 to 50 slm.\n",
      "[3] BLEU: 2.89177102115629e-155, ROUGE-L: 0.16470588235294117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     51\u001b[39m prompt = textwrap.dedent(\n\u001b[32m     52\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33m    [INST] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [/INST]\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     55\u001b[39m )\n\u001b[32m     56\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m gen = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m answer = tokenizer.decode(gen[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n\u001b[32m     59\u001b[39m gold = row[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/peft/peft_model.py:1875\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1873\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1874\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1875\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1877\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/generation/utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m output_hidden_states = (\n\u001b[32m    817\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    818\u001b[39m )\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    560\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    561\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    568\u001b[39m         position_embeddings,\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:334\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m residual = hidden_states\n\u001b[32m    333\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    337\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:172\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:494\u001b[39m, in \u001b[36mLinear4bit.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    492\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.base_layer(x, *args, **kwargs)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[32m    496\u001b[39m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[32m    497\u001b[39m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[32m    499\u001b[39m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[32m    500\u001b[39m     result = result.clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:482\u001b[39m, in \u001b[36mLinear4bit.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    480\u001b[39m     x = x.to(\u001b[38;5;28mself\u001b[39m.compute_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m bias = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias.to(\u001b[38;5;28mself\u001b[39m.compute_dtype)\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bnb.matmul_4bit(x, \u001b[38;5;28mself\u001b[39m.weight.t(), bias=bias, quant_state=\u001b[38;5;28mself\u001b[39m.weight.quant_state).to(inp_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm_rca/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1927\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1922\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1926\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1928\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1929\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def perplexity(eval_texts: List[str], model, tokenizer):\n",
    "    ppl_list = []\n",
    "\n",
    "    for text in eval_texts:\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        if not torch.isnan(loss):\n",
    "            ppl = torch.exp(loss).item()\n",
    "            ppl_list.append(ppl)\n",
    "\n",
    "    if len(ppl_list) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    return sum(ppl_list) / len(ppl_list)\n",
    "\n",
    "\n",
    "pretrined = False\n",
    "\n",
    "if pretrined==True:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_auth_token=token, device_map=\"auto\")\n",
    "    # Load LoRA adapter\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "model.eval()\n",
    "\n",
    "# 3-5 random chunks for perplexity\n",
    "eval_ds = load_dataset(\"json\", data_files=str(dataset_path), split=\"train\") #[:1%]\n",
    "sample_texts = [eval_ds[i][\"text\"] for i in range(min(5, len(eval_ds)))]\n",
    "ppl = perplexity(sample_texts, model, tokenizer)\n",
    "print(f\"[i] Domain PPL ≈ {ppl:.2f}\")\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "if eval_questions:\n",
    "    df = pd.read_csv(eval_questions)\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            [INST] {row['question']} [/INST]\n",
    "            \"\"\"\n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        gen = model.generate(**inputs, max_new_tokens=64, pad_token_id=tokenizer.eos_token_id)\n",
    "        answer = tokenizer.decode(gen[0], skip_special_tokens=True).strip()\n",
    "        gold = row[\"answer\"].strip()\n",
    "\n",
    "        bleu = sentence_bleu([gold.split()], answer.split())\n",
    "        rouge_l = scorer.score(gold, answer)['rougeL'].fmeasure\n",
    "        print(f\"[Answer]: {gold}\")\n",
    "        print(f\"[{idx}] BLEU: {bleu}, ROUGE-L: {rouge_l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1dd1cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sqlite3\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    return_full_text=False,\n",
    "    temperature=0.1,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9416b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GNN 결과 로드\n",
    "test_result = pd.read_csv(\"/home/mskim2/GDN/csv/swat/test_result.csv\")\n",
    "attention = pd.read_csv(\"/home/mskim2/GDN/csv/swat/attention_result.csv\")\n",
    "anomaly_score = pd.read_csv(\"/home/mskim2/GDN/csv/swat/anomaly_score.csv\")\n",
    "raw_df = pd.read_csv(\"/home/mskim2/GDN/data/swat/test.csv\")\n",
    "\n",
    "feature_file = open(f'/home/mskim2/GDN/data/swat/list.txt', 'r')\n",
    "feature_list = []\n",
    "for ft in feature_file:\n",
    "    feature_list.append(ft.strip())\n",
    "\n",
    "attack_point = pd.read_csv(\"/home/mskim2/GDN/attack_point.csv\")\n",
    "attack_point = attack_point.iloc[5:, -1].tolist()\n",
    "test_result['attack_point'] = attack_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e3cc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. true positive 시점 필터링\n",
    "tp_df = test_result[(test_result[\"ground truth label\"] == 1.0) & (test_result[\"model prediction\"] == 1.0)& (pd.notna(test_result[\"attack_point\"]))]\n",
    "tp_df = tp_df.drop(tp_df.index[68])\n",
    "\n",
    "# 4-1. DB 연결 (SQLite / CSV 예시)\n",
    "conn = sqlite3.connect(\"sensor_data.db\")\n",
    "def get_raw_sensor_data(sensor: str, time_idx: int, window: int = 10) -> str:\n",
    "    query = f\"\"\"\n",
    "        SELECT timestamp, value\n",
    "        FROM raw_data\n",
    "        WHERE sensor_id = '{sensor}'\n",
    "        AND time_index BETWEEN {time_idx - window} AND {time_idx + window}\n",
    "        ORDER BY time_index\n",
    "    \"\"\"\n",
    "    result = pd.read_sql(query, conn)\n",
    "    return result.to_string(index=False)\n",
    "\n",
    "def get_sensor_data_block(raw_df: pd.DataFrame, sensor: list, time_idx: int, window: int = 10) -> str:\n",
    "    start = max(0, time_idx - window)\n",
    "    end = min(len(raw_df), time_idx + window + 1)\n",
    "    block = raw_df.loc[start:end, sensor]\n",
    "    lines = [f\"{val}\" for idx, val in block.items()]\n",
    "    return \", \".join(lines)\n",
    "\n",
    "def get_attention_data_block(df, sensor, time_idx, window):\n",
    "    topk = 15\n",
    "    node_num = 51\n",
    "    block = df.loc[(time_idx)*node_num*topk:(time_idx+1)*node_num*topk, :].squeeze()\n",
    "    sensor_graph = {}\n",
    "    for _, row in block.iterrows():\n",
    "        source = row['source']\n",
    "        target = row['target']\n",
    "        attn = row['attention']\n",
    "\n",
    "        if source not in sensor_graph:\n",
    "            sensor_graph[source] = {}\n",
    "        \n",
    "        sensor_graph[source][target] = attn\n",
    "\n",
    "    return sensor_graph\n",
    "\n",
    "# 5. 도메인 매뉴얼 불러오기\n",
    "with open(\"./manual.txt\", \"r\") as f:\n",
    "    manual_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be137781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Index:  1538\n",
      "Top 3 Sensors:  ['FIT401', 'MV301', 'FIT501']\n",
      "\n",
      "--- Root Cause: FIT-401 ---\n",
      "\n",
      "--- Root Cause Analysis for time 1538 ---\n",
      "{\n",
      "  \"root_causes\": [\n",
      "    {\"cause\": \"FIT401 sensor anomaly\", \"evidence\": [\"FIT401\"], \"confidence\": 0.99},\n",
      "    {\"cause\": \"P101 actuator anomaly\", \"evidence\": [\"P101\"], \"confidence\": 0.95},\n",
      "    {\"cause\": \"MV101 actuator anomaly\", \"evidence\": [\"MV101\"], \"confidence\": 0.92}\n",
      "  ],\n",
      "  \"supporting_detail\": \"The anomaly is likely caused by a sensor or actuator malfunction in the ultrafiltration stage. The sensor data from FIT401 shows a sudden drop in water flow rate, which is not reflected in the other sensors. The attention weights from the Graph Neural Network indicate a strong correlation between FIT401 and P101, suggesting that P101 is likely to be involved in the anomaly. Additionally, the attention weights show a moderate correlation between MV101 and FIT401, indicating that MV101 may also be related to the anomaly. Further investigation is needed to determine the root cause of the anomaly.\"\n",
      "}\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@ ['FIT-401', 'P-101', 'MV-101'] FIT-401 @@@@@@@@@@@@@@@@@@@@@@\n",
      "Time Index:  11614\n",
      "Top 3 Sensors:  ['AIT504', 'FIT501', 'FIT401']\n",
      "\n",
      "--- Root Cause: AIT-504 ---\n",
      "\n",
      "--- Root Cause Analysis for time 11614 ---\n",
      "{\n",
      "  \"root_causes\": [\n",
      "    {\"cause\": \"High conductivity in water\", \"evidence\": [\"AIT504\"], \"confidence\": 0.99},\n",
      "    {\"cause\": \"High water level in the UF feed water tank\", \"evidence\": [\"LIT101\"], \"confidence\": 0.93},\n",
      "    {\"cause\": \"High water level in the UF feed water tank\", \"evidence\": [\"LIT101\"], \"confidence\": 0.93}\n",
      "  ],\n",
      "  \"supporting_detail\": \"The anomaly is likely caused by a high water level in the UF feed water tank (LIT101) and high conductivity in water (AIT504). The high water level could be due to a malfunction in the pump P101 or a blockage in the UF feed water tank. The high conductivity in water could be due to a malfunction in the reverse osmosis system or a contamination in the water. The attention weights show a strong correlation between LIT101 and AIT504, indicating that the model is using the water level in the UF feed water tank as a feature to predict the conductivity in water. The attention weights also show a strong correlation between LIT101 and MV101, indicating that the model is using the water level in the UF feed water tank as a feature to predict the state of the motorized valve MV101. The attention weights also show a strong correlation between AIT504 and P101, indicating that the model is using the conductivity in water as a feature to predict the state of the pump P101.\"\n",
      "}\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@ ['AIT-504', 'LIT-101', 'LIT-101'] AIT-504 @@@@@@@@@@@@@@@@@@@@@@\n",
      "Time Index:  13291\n",
      "Top 3 Sensors:  ['FIT501', 'FIT401', 'MV302']\n",
      "\n",
      "--- Root Cause: AIT-502,P-501,UV-401 ---\n",
      "\n",
      "--- Root Cause Analysis for time 13291 ---\n",
      "{\n",
      "  \"root_causes\": [\n",
      "    {\"cause\": \"Possible attack on P101\", \"evidence\": [\"P101\"], \"confidence\": 1.0},\n",
      "    {\"cause\": \"Possible attack on P101\", \"evidence\": [\"P101\"], \"confidence\": 1.0},\n",
      "    {\"cause\": \"Possible attack on P101\", \"evidence\": [\"P101\"], \"confidence\": 1.0}\n",
      "  ],\n",
      "  \"supporting_detail\": \"The anomaly is detected in P101, which is a pump. The attention weights show that LIT101 has a high attention weight to P101, indicating a strong correlation between the two sensors. The value of LIT101 is also anomalous. Therefore, it is likely that the anomaly is caused by an attack on P101.\"\n",
      "}\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@ ['P-101', 'P-101', 'P-101'] AIT-502,P-501,UV-401 @@@@@@@@@@@@@@@@@@@@@@\n",
      "Time Index:  23171\n",
      "Top 3 Sensors:  ['P502', 'UV401', 'P206']\n",
      "\n",
      "--- Root Cause: P-302 ---\n",
      "\n",
      "--- Root Cause Analysis for time 23171 ---\n",
      "{\n",
      "  \"root_causes\": [\n",
      "    {\"cause\": \"Chemical dosing pump P206 is not working\", \"evidence\": [\"P206\"], \"confidence\": 0.99},\n",
      "    {\"cause\": \"Reverse osmosis system is not working\", \"evidence\": [\"P501\"], \"confidence\": 0.98},\n",
      "    {\"cause\": \"UV dechlorination unit is not working\", \"evidence\": [\"UV401\"], \"confidence\": 0.97}\n",
      "  ],\n",
      "  \"supporting_detail\": \"The anomaly is likely caused by a malfunction of one of the critical components in the water treatment system. The attention weights indicate that the sensors related to the reverse osmosis system (PIT501, PIT502, PIT503) and the UV dechlorination unit (UV401) have high attention weights with the sensor P502, which is a pressure sensor after the reverse osmosis membrane. Additionally, the sensor P501, which is a pressure sensor before the reverse osmosis membrane, has a high attention weight with the sensor P502. Furthermore, the sensor P206, which is a chemical dosing pump, has a high attention weight with the sensor P502. The high attention weights of these sensors with P502 indicate that the model is paying attention to these sensors when predicting the value of P502. The high attention weight of P206 with P502 suggests that the model is using the information from P206 to predict the value of P502. However, the value of P206 is not changing, which indicates that the pump is not working. Therefore, the most likely root cause of the anomaly is that the chemical dosing pump P206 is not working.\"\n",
      "}\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@ ['P-206', 'P-501', 'UV-401'] P-302 @@@@@@@@@@@@@@@@@@@@@@\n",
      "Time Index:  43654\n",
      "Top 3 Sensors:  ['FIT502', 'P502', 'UV401']\n",
      "\n",
      "--- Root Cause: FIT-502,P-501 ---\n",
      "\n",
      "--- Root Cause Analysis for time 43654 ---\n",
      "{\n",
      "  \"root_causes\": [\n",
      "    {\"cause\": \"Possible attack on the water treatment system\", \"evidence\": [\"FIT502\", \"FIT504\", \"PIT503\"], \"confidence\": 0.9},\n",
      "    {\"cause\": \"Possible attack on the water treatment system\", \"evidence\": [\"FIT502\", \"FIT504\", \"PIT503\"], \"confidence\": 0.9},\n",
      "    {\"cause\": \"Possible attack on the water treatment system\", \"evidence\": [\"FIT502\", \"FIT504\", \"PIT503\"], \"confidence\": 0.9}\n",
      "  ],\n",
      "  \"supporting_detail\": \"The anomaly is likely caused by a coordinated attack on the water treatment system. Sensors FIT502 and FIT504 are measuring the water flow in the ultrafiltration stage and the reverse osmosis stage, respectively. The values of these sensors are significantly different from the expected values. Additionally, the pressure sensor PIT503 in the reverse osmosis stage is also showing an anomaly. This suggests that the attacker may have manipulated the system to disrupt the normal operation of the water treatment process.\"\n",
      "}\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@ ['FIT-502', 'FIT-502', 'FIT-502'] FIT-502,P-501 @@@@@@@@@@@@@@@@@@@@@@\n",
      "Time Index:  43814\n",
      "Top 3 Sensors:  ['P201', 'FIT501', 'FIT401']\n",
      "\n",
      "--- Root Cause: AIT-502,FIT-401 ---\n",
      "\n",
      "--- Root Cause Analysis for time 43814 ---\n",
      "{\n",
      "  \"root_causes\": [\n",
      "    {\"cause\": \"Chemical dosing pump P201 was turned off\", \"evidence\": [\"P201\"], \"confidence\": 0.9},\n",
      "    {\"cause\": \"Chemical dosing pump P201 was turned off\", \"evidence\": [\"P201\"], \"confidence\": 0.9},\n",
      "    {\"cause\": \"Chemical dosing pump P201 was turned off\", \"evidence\": [\"P201\"], \"confidence\": 0.9}\n",
      "  ],\n",
      "  \"supporting_detail\": \"The anomaly was detected in sensor P201, which measures the flow of chemicals in the water treatment process. The attention weights show that sensor P201 has a strong correlation with sensor P205, which measures the flow of water after the ultrafiltration stage. The sensor values of P201 and P205 show a sudden drop around the anomaly, indicating that the chemical dosing pump P201 was turned off. This is a plausible root cause of the anomaly, as the water treatment process relies on the correct dosing of chemicals to maintain water quality.\"\n",
      "}\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@ ['P-201', 'P-201', 'P-201'] AIT-502,FIT-401 @@@@@@@@@@@@@@@@@@@@@@\n",
      "Time Index:  43862\n",
      "Top 3 Sensors:  ['P201', 'FIT501', 'MV301']\n",
      "\n",
      "--- Root Cause: FIT-401 ---\n",
      "\n",
      "--- Root Cause Analysis for time 43862 ---\n",
      "{\n",
      "  \"root_causes\": [\n",
      "    {\"cause\": \"Chemical dosing pump P201 was not working.\", \"evidence\": [\"P201\"], \"confidence\": 0.99},\n",
      "    {\"cause\": \"The water level in the UF feed water tank was too high.\", \"evidence\": [\"LIT301\"], \"confidence\": 0.93},\n",
      "    {\"cause\": \"The ultrafiltration stage was not working.\", \"evidence\": [\"FIT301\"], \"confidence\": 0.92}\n",
      "  ],\n",
      "  \"supporting_detail\": \"The anomaly was detected in sensor P201, which is a chemical dosing pump. The attention weights show that P201 has a high attention weight with sensor LIT301, which measures the water level in the UF feed water tank. The water level in the tank was too high, which indicates that the ultrafiltration stage was not working properly. The ultrafiltration stage is controlled by sensor FIT301, which measures the flow rate in the ultrafiltration stage. The flow rate was too low, indicating that the ultrafiltration stage was not working properly. The attention weights also show that P201 has a high attention weight with sensor MV301, which is a motorized valve. However, the value of MV301 was 0.5, which is normal. Therefore, the most likely root cause of the anomaly is that the chemical dosing pump P201 was not working.\"\n",
      "}\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@ ['P-201', 'LIT-301', 'FIT-301'] FIT-401 @@@@@@@@@@@@@@@@@@@@@@\n",
      "3 4\n",
      "Accuracy:  0.42857142857142855\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "root = None\n",
    "\n",
    "# 6. 루트 원인 분석 루프\n",
    "slide_win = 5\n",
    "window = 30\n",
    "prev_value = None\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for _, row in tp_df.iterrows():\n",
    "    current_value = row['attack_point']\n",
    "    if current_value != prev_value:\n",
    "        time_idx = int(row[\"timestamp\"] + slide_win)\n",
    "        print(\"Time Index: \", time_idx)\n",
    "        sensors_scores = [s.strip() for s in row[[\"1\", \"2\", \"3\"]].tolist()]\n",
    "        sensors = [s.split(\":\")[0] for s in sensors_scores]\n",
    "        print(\"Top 3 Sensors: \", sensors)\n",
    "\n",
    "        output_json = {\n",
    "            \"raw_data\": {},\n",
    "            \"anomaly_scores\": {},\n",
    "            \"attention\": {},\n",
    "        }\n",
    "\n",
    "        for sensor in feature_list:\n",
    "            anomaly = get_sensor_data_block(anomaly_score, sensor, time_idx, window=window)\n",
    "            output_json[\"anomaly_scores\"][sensor] = anomaly\n",
    "\n",
    "        for sensor in feature_list:\n",
    "            raw = get_sensor_data_block(raw_df, sensor, time_idx, window=window)\n",
    "            output_json[\"raw_data\"][sensor] = raw\n",
    "\n",
    "        anomaly = get_attention_data_block(attention, sensor, time_idx, window=window)\n",
    "        output_json[\"attention\"] = anomaly\n",
    "\n",
    "        root = row['attack_point']\n",
    "\n",
    "        messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an expert in root cause analysis for cyber-physical systems, especially industrial water treatment systems. Your task is to identify plausible root causes of detected anomalies. Use domain knowledge and respond concisely and clearly.\n",
    "\n",
    "        TASK:\n",
    "        1. Read the files provided in subsequent messages:\n",
    "        - Sensor Manual: A textual guide containing descriptions of each sensor and actuator, their intended functionality.\n",
    "        - Raw Sensor Data: A dictionary mapping each sensor name to a string containing comma-separated raw data over a time window (±30 time steps from the detected anomaly).\n",
    "        - Attention Weights: A dictionary where each key is a source sensor name, and its value is another dictionary mapping target sensor names to attention values (floats between 0 and 1). The attention values represents the influence or correlation strength from the source sensor to the target sensor as learned by the Graph Neural Network.\n",
    "        2. Return a JSON object with:\n",
    "        {\n",
    "            \"root_causes\": [\n",
    "            {\"cause\": str, \"evidence\": [sensor_id], \"confidence\": 0-1 float}\n",
    "            ],\n",
    "            \"supporting_detail\": str (<=150 tokens)\n",
    "        }\n",
    "        CONSTRAINTS:\n",
    "        - Use only the given data; do not hallucinate unseen equipment.\n",
    "        - Be concise; no markdown, no additional text outside the JSON.\n",
    "        - Identify the most plausible root cause by considering abnormal changes in raw data or attention weights, as well as the inter-sensor relationships and the system’s operational flow.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "        Top 3 Sensors (by anomaly score): {', '.join(sensors)}\n",
    "\n",
    "        Sensor Manual:\n",
    "        {manual_text}\n",
    "\n",
    "        Raw Sensor Data (±{window} points around anomaly):\n",
    "        {output_json['raw_data']}\n",
    "\n",
    "        Attention Weights:\n",
    "        {output_json['attention']}\n",
    "\n",
    "        Please analyze and explain the most likely root cause of the anomaly (maximum 3 sensors). Respond only with the required JSON output.\"\"\"\n",
    "        }\n",
    "        ]\n",
    "\n",
    "        response = llm_pipeline(messages, max_new_tokens=512)\n",
    "        print(\"\\n--- Root Cause:\", root, '---')\n",
    "        print(f\"\\n--- Root Cause Analysis for time {time_idx} ---\\n{response[0]['generated_text']}\\n\")\n",
    "        parsed = json.loads(response[0]['generated_text'])\n",
    "        \n",
    "        predicted_root_sensors = []\n",
    "        for i in range(len(parsed['root_causes'])):\n",
    "            predicted_root = parsed['root_causes'][i]['evidence'][0]\n",
    "            predicted_root = re.sub(r\"([A-Za-z]+)(\\d+)\", r\"\\1-\\2\", predicted_root)            \n",
    "            predicted_root_sensors.append(predicted_root)\n",
    "\n",
    "        acc = 1 if any(sensor in root for sensor in predicted_root_sensors) else 0\n",
    "\n",
    "        if acc:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@\", predicted_root_sensors, root, \"@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "    prev_value = current_value\n",
    "\n",
    "print(correct, incorrect)\n",
    "print(\"Accuracy: \", correct / (correct + incorrect))\n",
    "print('------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b483c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408bb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923695a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
