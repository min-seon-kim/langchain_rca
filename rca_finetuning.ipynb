{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# --- PDF → text utility (PyMuPDF)\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    fitz = None  # Will error later if preprocess is used\n",
    "\n",
    "# --- Deduplication\n",
    "import text_dedup.minhash\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# --- Hugging Face & PEFT\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39dbc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess config\n",
    "pdf_dir = './pdf'\n",
    "out_dir = './data'\n",
    "min_tokens = 128\n",
    "max_tokens = 2048\n",
    "overlap = 256\n",
    "\n",
    "# train config\n",
    "dataset_path = './data/train.jsonl'\n",
    "output_dir = './checkpoints'\n",
    "num_epochs = 3\n",
    "per_device_train_batch = 2\n",
    "grad_accum = 4\n",
    "lr = 2e-4\n",
    "warmup_steps = 100\n",
    "lora_r = 64\n",
    "lora_alpha = 128\n",
    "lora_dropout = 0.05\n",
    "max_seq_len = 2048\n",
    "fp16 = True  # 또는 False\n",
    "\n",
    "# evaluate config\n",
    "adapter_dir = './checkpoints/adapter'\n",
    "eval_dataset_path = './data/eval.json'\n",
    "eval_questions = './data/questions.csv'  # CSV with columns 'question','answer_regex'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff7320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Extracted pdf/Analysis of silane and nitrous oxide produced plasma enhanced chemical vapor deposition simulation.pdf\n",
      "[+] Extracted pdf/Analysis-of-the-synergetic-effect-of-process-parameters-of-h_2025_Diamond-an.pdf\n",
      "[+] Extracted pdf/A-novel-physical-vapor-deposition-setup-applying-high-frequency-cur_2025_Vac.pdf\n",
      "[+] Extracted pdf/A-review-of-comprehensive-utilization-of-biomass-to-s_2024_Journal-of-Analyt.pdf\n",
      "[+] Extracted pdf/A-transport-kinetic-model-development-for-polysili_2024_International-Journa.pdf\n",
      "[+] Extracted pdf/Centimeter-level-MoS2-films-with-controllable-number-of-layers-by-f_2023_Vac.pdf\n",
      "[+] Extracted pdf/Characteristics-of-Single-Crystalline-Rutile-GeO2-Film-Gro_2025_Journal-of-A.pdf\n",
      "[+] Extracted pdf/Chemical vapor deposition growth of boron incorporated graphitic carbon nitride film for carbon based semiconductor systems.pdf\n",
      "[+] Extracted pdf/Chemical-vapor-deposited-nanocarbon-Fe-Al2O3-composi_2025_Materials-Chemistr.pdf\n",
      "[+] Extracted pdf/Chemical-vapor-deposition-grown-graphene-transparent-con_2024_Renewable-and-.pdf\n",
      "[+] Extracted pdf/Chemical-vapor-deposition-grown-single-layer-graphene-support_2024_Journal-o.pdf\n",
      "[+] Extracted pdf/Chemical-vapor-deposition-growth-of-large-area-molyb_2024_Nano-Structures---.pdf\n",
      "[+] Extracted pdf/Chemical-vapor-deposition-of-graphene-and-its-character_2024_Current-Applied.pdf\n",
      "[+] Extracted pdf/Chemiresistive-SnO2-based-hydrogen-sensor-fabricated-b_2025_Sensors-and-Actu.pdf\n",
      "[+] Extracted pdf/Co-deposition-of-TaC-and-SiC-by-chemical-vapor-deposit_2024_Surface-and-Coat.pdf\n",
      "[+] Extracted pdf/Completely Filling of Through-Silicon-Vias with High Aspect Ratio by High Cavity Physical Vapor Deposition and Electroplating.pdf\n",
      "[+] Extracted pdf/Composition tunable inorganic Lead Halide Perovskites microstructures synthesized by single and two-step chemical vapor deposition methods.pdf\n",
      "[+] Extracted pdf/Controllable-growth---In2Se3-flakes-by-chemical-vapor-d_2023_Results-in-Phys.pdf\n",
      "[+] Extracted pdf/Controlled-epitaxial-growth-of-strain-induced-large-area-bil_2024_Materials-.pdf\n",
      "[+] Extracted pdf/Controlled-growth-of-two-dimensional-MoS2-WSe2-heter_2025_Materials-Science-.pdf\n",
      "[+] Extracted pdf/Crying-Wulff-on-vapor-solid-distributions-of-crystallogen-c_2023_Journal-of-.pdf\n",
      "[+] Extracted pdf/CsPbBr3-based-photoanode-prepared-by-single-step-Chemical-v_2024_Applied-Sur.pdf\n",
      "[+] Extracted pdf/CW Lasing at 1.35 --mu-m From Ten InAs-Sb--GaAs Quantum-Dot Layers Grown by Metal-Organic Chemical Vapor Deposition.pdf\n",
      "[+] Extracted pdf/Deposition-selectivity-on-oxide-versus-metal-surfaces-via-cat_2025_Applied-S.pdf\n",
      "[+] Extracted pdf/Dimerization-equilibrium-of-group-13-precursors-_2024_Computational-and-Theo.pdf\n",
      "[+] Extracted pdf/Ecofriendly-alkali-metal-cations-diffusion-improves-fabrication_2023_Environ.pdf\n",
      "[+] Extracted pdf/Effect-of-linear-antenna-chemical-vapor-deposition-process-_2024_Journal-of-.pdf\n",
      "[+] Extracted pdf/Effect-of-substrate-temperature-on-silicon-Oxycarbide-thin-fil_2025_Thin-Sol.pdf\n",
      "[+] Extracted pdf/Effects-of-carbon-nanoparticle-insertion-on-stress-reducti_2024_Diamond-and-.pdf\n",
      "[+] Extracted pdf/Electrochemical-polishing-of-chemical-vapor-deposited-niob_2023_Thin-Solid-F.pdf\n",
      "[+] Extracted pdf/Enhanced-solar-water-splitting-using-bismuth-ferrite_2024_Materials-Science-.pdf\n",
      "[+] Extracted pdf/Enhancement-of-crystalline-quality-of-homoepitaxial--100--_2025_Diamond-and-.pdf\n",
      "[+] Extracted pdf/Enhancing-the-thermal-stability-of-active-contacts-in-AlGaN-G_2024_Current-A.pdf\n",
      "[+] Extracted pdf/Epitaxially deposited SrVO-sub 3- conducting films by laser ablation and metal organic chemical vapor deposition.pdf\n",
      "[+] Extracted pdf/Evaluation of Ga-Sn-O films fabricated using mist chemical vapor deposition.pdf\n",
      "[+] Extracted pdf/Exploring-the-growth-and-optoelectronic-properties-of-PtSe_2025_Surfaces-and.pdf\n",
      "[+] Extracted pdf/Flexible-and-lightweight-graphene-grown-by-rapid-thermal-proce_2023_New-Carb.pdf\n",
      "[+] Extracted pdf/Growth of GaN nanowires on Si substrate using Ni catalyst in vertical chemical vapor deposition reactor.pdf\n",
      "[+] Extracted pdf/04_34015_rashid.pdf\n",
      "[+] Extracted pdf/20220728105632_74178.pdf\n",
      "[+] Extracted pdf/Growth-dynamics-dependence-of-high-quality---Ga2O3-thin-films-prepa_2025_Vac.pdf\n",
      "[+] Extracted pdf/Growth-of-bilayer-MoS2-flakes-by-reverse-flow-chemical-va_2023_Materials-Let.pdf\n",
      "[+] Extracted pdf/H2-mediated-reduction-of-GeO2-and-chemical-vapor-deposition-_2023_Thin-Solid.pdf\n",
      "[+] Extracted pdf/--hbox-SnO-_-2--  Nanorods Prepared by Inductively Coupled Plasma-Enhanced Chemical Vapor Deposition.pdf\n",
      "[+] Extracted pdf/Heavy-phosphorus-doping-of-diamond-by-hot-filament-_2023_Diamond-and-Related.pdf\n",
      "[+] Extracted pdf/High-light-yield-and-fast-response---Ga2O3-Al2O3-thick-film-sc_2024_Material.pdf\n",
      "[+] Extracted pdf/High-performance-solar-blind-photodetector-based-on-Si-d_2024_Journal-of-All.pdf\n",
      "[+] Extracted pdf/High-quality-MoS2-monolayers-with-largely-enhanced-electrical_2024_Applied-S.pdf\n",
      "[+] Extracted pdf/High-rate-growth-of-single-crystal-diamond-with-an-atomically-_2022_Thin-Sol.pdf\n",
      "[+] Extracted pdf/High-reflectance dielectric mirrors deposited by plasma-enhanced chemical vapor deposition on GaAs-AlGaAs semiconductor lasers with inductively coupled plasma etched facets.pdf\n",
      "[+] Extracted pdf/High-throughput-thermodynamic-analysis-of-epitaxial-growt_2024_Materials-Tod.pdf\n",
      "[+] Extracted pdf/Hot wire chemical vapor deposited boron carbide thin film-c-silicon diode for neutron detection application.pdf\n",
      "[+] Extracted pdf/III-V semiconductor heterojunction devices grown by metalorganic chemical vapor deposition.pdf\n",
      "[+] Extracted pdf/III-V Semiconductor Quantum-Well Devices Grown by Metalorganic Chemical Vapor Deposition.pdf\n",
      "[+] Extracted pdf/Imaging-neutron-radiation-induced-defects-in-single-crys_2025_Diamond-and-Re.pdf\n",
      "[+] Extracted pdf/Impact-of-humidity-on-long-term-stability-of-HfS2-grow_2025_Materials-Scienc.pdf\n",
      "[+] Extracted pdf/Improved GaN-Based LED Light Extraction Efficiencies via Selective MOCVD Using Peripheral Microhole Arrays.pdf\n",
      "[+] Extracted pdf/Improved-heteroepitaxy-of---Ga2O3-on-c-plane-sapphire-by-initia_2024_Thin-So.pdf\n",
      "[+] Extracted pdf/Improved-photoelectrochemical-performance-of-tungsten-catalyzed-g_2024_Thin-.pdf\n",
      "[+] Extracted pdf/In-situ-fabrication-of-dendrite-like-SiC-heterostructure-re_2025_Ceramics-In.pdf\n",
      "[+] Extracted pdf/In-situ-growth-of-ZnO-ZnWO4-heterojunction-with-flower-like_2025_Applied-Sur.pdf\n",
      "[+] Extracted pdf/In-situ-microscopic-observation-of-single-crystal-dia_2025_Diamond-and-Relat.pdf\n",
      "[+] Extracted pdf/In-situ-monitoring-of-industrial-scale-chemical-vapor-dep_2024_Surfaces-and-.pdf\n",
      "[+] Extracted pdf/Investigation of silicon-germanium metal-oxide-semiconductor field-effect transistors grown by laser-assisted plasma-enhanced chemical vapor deposition.pdf\n",
      "[+] Extracted pdf/Investigation-on-the-effect-of-chemical-mechanical-polishing-_2024_Surfaces-.pdf\n",
      "[+] Extracted pdf/Large-scale-growth-of-MoS2-hybrid-layer-by-chemical-vap_2024_Microelectronic.pdf\n",
      "[+] Extracted pdf/Low-temperature growth of Si-nanostructures by laser assistance of conventional plasma enhanced chemical vapor deposition.pdf\n",
      "[+] Extracted pdf/Low-temperature-chemical-vapor-deposition-as-a-sustainable-met_2023_Material.pdf\n",
      "[+] Extracted pdf/Lyophilization-strategy-to-synthesize-ultrafine-NaCl-template_2024_Surfaces-.pdf\n",
      "[+] Extracted pdf/Machine-learning-models-in-the-process-of-metal-organic-c_2024_Materials-Tod.pdf\n",
      "[+] Extracted pdf/Magnetic-properties-of-Fe-parylene-soft-magnetic-_2023_Journal-of-Magnetism-.pdf\n",
      "[+] Extracted pdf/MaterialsscienceandprocesstechnologyseriesHughOPierson-Handbookofchemicalvapordepostioni.e.depositionCVD_principlestechnologyandapplications-NoyesPublications_WilliamAn.pdf\n",
      "[+] Extracted pdf/Mechanistic-insights-into-diffusion-controlled-2D-WSe2-growth-via-_2025_Flat.pdf\n",
      "[+] Extracted pdf/Metalorganic chemical vapor deposition for optoelectronic devices.pdf\n",
      "[+] Extracted pdf/Microstructure-evolution-of-carbon-films--grown-by-physi_2024_Materials-Chem.pdf\n",
      "[+] Extracted pdf/Mixed-phase-iron-oxides-thin-layers-by-atmospheric-pressure-chemi_2024_Metho.pdf\n",
      "[+] Extracted pdf/Model reduction for a tungsten chemical vapor deposition system.pdf\n",
      "[+] Extracted pdf/Modulating-growth-of-graphene-on-sapphire-by-chemica_2024_Journal-of-Crystal.pdf\n",
      "[+] Extracted pdf/Monolithically-grown-CSPbBr3-by-chemical-vapor-deposit_2024_Chemical-Enginee.pdf\n",
      "[+] Extracted pdf/Multifunctional-prospects-of-physical-vapor-depo_2025_Journal-of-Science--Ad.pdf\n",
      "[+] Extracted pdf/Multi-objective-optimization-of-4H-SiC-homoepitaxy-chemica_2025_Materials-To.pdf\n",
      "[+] Extracted pdf/Multiscale-multiphysics-predictive-modeling-of-chemical-v_2025_Chemical-Engi.pdf\n",
      "[+] Extracted pdf/Numerical-analysis-of-the-use-of-multiple-inlet-plates-to-impro_2024_Results.pdf\n",
      "[+] Extracted pdf/Numerical-investigation-on-the-effect-of-growth-conditions_2025_Journal-of-C.pdf\n",
      "[+] Extracted pdf/Open-ended-chemical-vapor-transport-method-for-growth-of-t_2025_Materials-To.pdf\n",
      "[+] Extracted pdf/Optical-characterization-of-SnS-nanowires-by-chemical-vapor_2024_Chemical-Ph.pdf\n",
      "[+] Extracted pdf/Optical-properties-of-carbon-nitride-thin-films-_2025_Materials-Science-in-S.pdf\n",
      "[+] Extracted pdf/Optimization-of-growth-conditions-for-direct-synthesis-of-SnSe2_2025_Optical.pdf\n",
      "[+] Extracted pdf/Optimizing the double-cap procedure for InAs-InGaAsP-InP quantum dots by metal-organic chemical vapor deposition.pdf\n",
      "[+] Extracted pdf/Optimizing-chemical-vapor-deposition-reactor-design-and-thickness-u_2025_Vac.pdf\n",
      "[+] Extracted pdf/Optimizing-electromagnetic-interference-shielding-of-flexible_2025_Applied-S.pdf\n",
      "[+] Extracted pdf/Optimizing-the-chemical-vapor-deposition-process-of-4H-S_2024_Case-Studies-i.pdf\n",
      "[+] Extracted pdf/Oxygen-vacancies-modulating-performance-for-Ga2O3-solar-b_2024_Materials-Tod.pdf\n",
      "[+] Extracted pdf/Phase-controlled-epitaxy-of-wurtzite-ZnS-thin-films-by-metal_2025_Thin-Solid.pdf\n",
      "[+] Extracted pdf/Phase-controlled-growth-of-indium-selenide-by-metalorg_2024_Journal-of-Cryst.pdf\n",
      "[+] Extracted pdf/Photodetector-based-on-2H-WSe2-grown-by-physical-vapor-de_2024_Materials-Let.pdf\n",
      "[+] Extracted pdf/Photodetectors-based-on-chemical-vapor-deposition-or-liquid-_2023_Optical-Ma.pdf\n",
      "[+] Extracted pdf/Post-growth-annealing-effect-of-Li-doped-NiO-thin-f_2024_Materials-Science-a.pdf\n",
      "[+] Extracted pdf/Precise-surface-engineering--Leveraging-chemical-vapor-deposition-f_2024_Hel.pdf\n",
      "[+] Extracted pdf/Preparation-and-energy-band-analysis-of-graphene-diamond-_2025_Diamond-and-R.pdf\n",
      "[+] Extracted pdf/Preparation-of-Al-doped-SnO2-thin-films-via-ultrasonic-mist-_2025_Physica-B-.pdf\n",
      "[+] Extracted pdf/Preparation-of---Ga2O3---Ga2O3-type-II-phase-junctions-by-_2025_Ceramics-Int.pdf\n",
      "[+] Extracted pdf/Pulsed-chemical-vapor-deposition-for-crystalline-aluminum-nitri_2023_Thin-So.pdf\n",
      "[+] Extracted pdf/Reaction-dynamics-simulation-of-MoO2-cluster-precursor-with-m_2024_Applied-S.pdf\n",
      "[+] Extracted pdf/Reactive Force-Field Molecular Dynamics Study of the Silicon-Germanium Deposition Processes by Plasma Enhanced Chemical Vapor Deposition.pdf\n",
      "[+] Extracted pdf/Research on Transfer-Free Graphene Field-Effect Transistors Made by Plasma-Enhanced Chemical Vapor Deposition.pdf\n",
      "[+] Extracted pdf/Research-progress-in-chemical-vapor-deposition-for-high-te_2025_Composites-P.pdf\n",
      "[+] Extracted pdf/Reviewing-two-dimensional--2D--transition-metal-di-tellur_2025_Materials-Sci.pdf\n",
      "[+] Extracted pdf/Review-on-the-of-physical-vapor-deposition-on-imminent-c_2024_Materials-Toda.pdf\n",
      "[+] Extracted pdf/Room-temperature continuous-wave operation of 1.24--spl mu-m GaInNAs lasers grown by metal-organic chemical vapor deposition.pdf\n",
      "[+] Extracted pdf/Salt-promoted-growth-of-monolayer-tungsten-disulfide-on-hexa_2022_Applied-Su.pdf\n",
      "[+] Extracted pdf/Sapphire-optical-fiber-with--BN-SiBCN-n-coating-prepared-by-_2024_Ceramics-I.pdf\n",
      "[+] Extracted pdf/Sulfurization-dependency-of-WS2-crystal-orientation-on-s_2025_Journal-of-Sol.pdf\n",
      "[+] Extracted pdf/Supported-catalysts-derived-from-cobalt-phyllosilicates-for-chemica_2023_Car.pdf\n",
      "[+] Extracted pdf/Sustainable-vapor-phase-deposition-and-applications_2025_Separation-and-Puri.pdf\n",
      "[+] Extracted pdf/Synthesis-of-SnO2-nanowires-using-thermal-chemical-vapor-d_2023_Journal-of-A.pdf\n",
      "[+] Extracted pdf/The-effect-of-the-precursors-and-chemical-vapor-depositio_2025_Surface-and-C.pdf\n",
      "[+] Extracted pdf/The-microstructural-evolution-of-isotropic-graphite-_2024_Journal-of-Alloys-.pdf\n",
      "[+] Extracted pdf/Thermodynamic-analysis-of-phase-and-composition-during-the-_2025_Journal-of-.pdf\n",
      "[+] Extracted pdf/Tin-based-perovskite-films-fabricated-by-chemical-vapor-depo_2024_Chemical-P.pdf\n",
      "[+] Extracted pdf/Ultrasonic Assisted Mist Chemical Vapor Deposition Deposited MgO for AlGaNGaN Metal-Insulator-Semiconductor Ultraviolet Photodetector.pdf\n",
      "[+] Extracted pdf/Vapor-phase-epitaxial-growth-of-ultrathin-Nonlayered-CoSe_2024_Applied-Surfa.pdf\n",
      "[+] Extracted pdf/4H-SiC-trench-filling-by-chemical-vapor-deposition-using_2023_Journal-of-Cry.pdf\n",
      "[+] Extracted pdf/A-first-principles-investigation-into-the-adsorption-behavio_2024_Materials-.pdf\n",
      "[+] Extracted pdf/A-multiphysics-model-of-TCS-C2H4-H2-system-involving-che_2025_Journal-of-Man.pdf\n",
      "[i] Deduplicated: 125 → 125 docs\n",
      "[i] Final chunks: 636\n",
      "[✓] Saved data/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract raw text from a single PDF page-by-page via PyMuPDF.\"\"\"\n",
    "    if fitz is None:\n",
    "        raise RuntimeError(\"PyMuPDF not installed. pip install pymupdf\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_chunks = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\")\n",
    "        text_chunks.append(page_text)\n",
    "    raw = \"\\n\".join(text_chunks)\n",
    "    return raw\n",
    "\n",
    "SECTION_PATTERNS = [\n",
    "    re.compile(r\"^references$\", re.I),\n",
    "    re.compile(r\"^bibliography$\", re.I),\n",
    "    re.compile(r\"^acknowledg(e)?ments?$\", re.I),\n",
    "]\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove references/acknowledgment sections & excessive blank lines.\"\"\"\n",
    "    lines = [l.strip() for l in text.splitlines()]\n",
    "    cleaned: List[str] = []\n",
    "    skip = False\n",
    "    for ln in lines:\n",
    "        if any(p.match(ln.lower()) for p in SECTION_PATTERNS):\n",
    "            skip = True\n",
    "        if not skip and ln:\n",
    "            cleaned.append(ln)\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "\n",
    "def chunk_text(text: str, tokenizer, max_tokens: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Slice long text into overlapping chunks by token count.\"\"\"\n",
    "    tokens = tokenizer(text)[\"input_ids\"]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk_tokens = tokens[i : i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        if len(chunk_tokens) >= 32:  # minimal meaningful length\n",
    "            chunks.append(chunk_text)\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "pdf_dir = Path(pdf_dir)\n",
    "out_dir = Path(out_dir)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(\"token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=token)\n",
    "\n",
    "raw_records = []\n",
    "for pdf_path in pdf_dir.rglob(\"*.pdf\"):\n",
    "    raw = extract_text_from_pdf(pdf_path)\n",
    "    cleaned = clean_text(raw)\n",
    "    raw_records.append({\"doc_id\": pdf_path.stem, \"text\": cleaned})\n",
    "    print(f\"[+] Extracted {pdf_path}\")\n",
    "\n",
    "# Deduplicate\n",
    "threshold = 0.88\n",
    "num_perm = 128\n",
    "\n",
    "texts = [r[\"text\"] for r in raw_records]\n",
    "\n",
    "minhashes = []\n",
    "for text in texts:\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in text.split():\n",
    "        m.update(word.encode('utf8'))\n",
    "    minhashes.append(m)\n",
    "\n",
    "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "unique_indices = []\n",
    "seen = set()\n",
    "\n",
    "for i, m in enumerate(minhashes):\n",
    "    duplicates = lsh.query(m)\n",
    "    if not duplicates:\n",
    "        lsh.insert(f\"m{i}\", m)\n",
    "        unique_indices.append(i)\n",
    "\n",
    "unique_records = [raw_records[i] for i in unique_indices]\n",
    "print(f\"[i] Deduplicated: {len(texts)} → {len(unique_records)} docs\")\n",
    "\n",
    "# Chunking\n",
    "all_chunks = []\n",
    "for rec in unique_records:\n",
    "    chunks = chunk_text(rec[\"text\"], tokenizer, max_tokens, overlap)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\"text\": chunk, \"source\": f\"{rec['doc_id']}§{idx}\"})\n",
    "\n",
    "# Filter by min_tokens\n",
    "min_toks = min_tokens\n",
    "def token_len(example):\n",
    "    return len(tokenizer(example[\"text\"])[\"input_ids\"])\n",
    "\n",
    "all_chunks = [c for c in all_chunks if token_len(c) >= min_toks]\n",
    "print(f\"[i] Final chunks: {len(all_chunks)}\")\n",
    "\n",
    "# Write JSONL\n",
    "jsonl_path = out_dir / \"train.jsonl\"\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in all_chunks:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"[✓] Saved {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7122d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3153e68aa98647d2bea00bd8dec2f6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1783566/3783776678.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 12:23, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.465500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.443900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.114400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-681c30dc-65cdefc3167d7349307abd24;a8e04409-ad94-40b8-8bc0-deaa7b01dbf2)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-681c31e3-2e95074624d7301b45b0a8a4;427c5f5a-20ed-4b11-b3a4-3f826e7079c3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-681c32ce-3f94ed56002311462d1f741a;8e7de142-1d2f-4649-9cbe-5aeb673f3130)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-681c32d0-784900690c1220ac2cc3105e;1f86bd6e-7541-4d37-bf5a-0aff1fda7171)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Training complete - adapter+tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path(dataset_path)\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(dataset_path)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=str(dataset_path), split=\"train\")\n",
    "\n",
    "with open(\"token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "# Tokenizer & model\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4-bit QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_auth_token=token, device_map=\"auto\")\n",
    "\n",
    "# PEFT config\n",
    "lora_cfg = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize dataset lazily\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_seq_len)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\", \"source\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=not fp16,\n",
    "    fp16=fp16,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"[✓] Training complete - adapter+tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca29d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='nltk')\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e45452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Domain PPL ≈ 2.29\n",
      "[0] BLEU: 7.884916681118857e-232, ROUGE-L: 0.07792207792207792\n",
      "[1] BLEU: 8.147480343967206e-232, ROUGE-L: 0.09523809523809523\n",
      "[2] BLEU: 6.573479617511883e-232, ROUGE-L: 0.0547945205479452\n",
      "[3] BLEU: 9.641193013181824e-232, ROUGE-L: 0.13513513513513514\n",
      "[4] BLEU: 0, ROUGE-L: 0.0547945205479452\n",
      "[5] BLEU: 6.995501686664742e-232, ROUGE-L: 0.08955223880597014\n",
      "[6] BLEU: 0, ROUGE-L: 0.027777777777777776\n",
      "[7] BLEU: 0, ROUGE-L: 0.0\n",
      "[8] BLEU: 8.928691163795855e-232, ROUGE-L: 0.08450704225352113\n",
      "[9] BLEU: 0, ROUGE-L: 0.0\n",
      "[10] BLEU: 0, ROUGE-L: 0.08823529411764705\n",
      "[11] BLEU: 6.784338172413661e-232, ROUGE-L: 0.02666666666666667\n",
      "[12] BLEU: 6.752107625974243e-232, ROUGE-L: 0.06060606060606061\n",
      "[13] BLEU: 0, ROUGE-L: 0.06896551724137931\n",
      "[14] BLEU: 0, ROUGE-L: 0.0\n",
      "[15] BLEU: 7.919883909890055e-232, ROUGE-L: 0.08955223880597016\n",
      "[16] BLEU: 6.630398171726777e-232, ROUGE-L: 0.02941176470588235\n",
      "[17] BLEU: 6.720628411503338e-232, ROUGE-L: 0.08695652173913043\n",
      "[18] BLEU: 6.466558133769387e-232, ROUGE-L: 0.02666666666666667\n",
      "[19] BLEU: 7.630227436828953e-232, ROUGE-L: 0.04761904761904761\n",
      "[20] BLEU: 7.955640502424632e-232, ROUGE-L: 0.08571428571428572\n",
      "[21] BLEU: 0, ROUGE-L: 0.0\n",
      "[22] BLEU: 6.885877678021066e-232, ROUGE-L: 0.0634920634920635\n",
      "[23] BLEU: 0, ROUGE-L: 0.02941176470588235\n",
      "[24] BLEU: 0, ROUGE-L: 0.0\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def perplexity(eval_texts: List[str], model, tokenizer):\n",
    "    ppl_list = []\n",
    "\n",
    "    for text in eval_texts:\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        if not torch.isnan(loss):\n",
    "            ppl = torch.exp(loss).item()\n",
    "            ppl_list.append(ppl)\n",
    "\n",
    "    if len(ppl_list) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    return sum(ppl_list) / len(ppl_list)\n",
    "\n",
    "\n",
    "pretrined = False\n",
    "\n",
    "if pretrined==True:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_auth_token=token, device_map=\"auto\")\n",
    "    # Load LoRA adapter\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "model.eval()\n",
    "\n",
    "# 3-5 random chunks for perplexity\n",
    "eval_ds = load_dataset(\"json\", data_files=str(dataset_path), split=\"train[:1%]\")\n",
    "sample_texts = [eval_ds[i][\"text\"] for i in range(min(5, len(eval_ds)))]\n",
    "ppl = perplexity(sample_texts, model, tokenizer)\n",
    "print(f\"[i] Domain PPL ≈ {ppl:.2f}\")\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "if eval_questions:\n",
    "    df = pd.read_csv(eval_questions)\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            [INST] {row['question']} [/INST]\n",
    "            \"\"\"\n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        gen = model.generate(**inputs, max_new_tokens=64, pad_token_id=tokenizer.eos_token_id)\n",
    "        answer = tokenizer.decode(gen[0], skip_special_tokens=True).strip()\n",
    "        gold = row[\"answer\"].strip()\n",
    "\n",
    "        bleu = sentence_bleu([gold.split()], answer.split())\n",
    "        rouge_l = scorer.score(gold, answer)['rougeL'].fmeasure\n",
    "        print(f\"[{idx}] BLEU: {bleu}, ROUGE-L: {rouge_l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdb775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9416b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cc49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d8347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] {preprocess} ...\n",
      "ipykernel_launcher.py: error: the following arguments are required: cmd\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mskim2/llm_rca/myenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3675: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "LoRA-DAPT Pipeline for Llama-3.1-8B-Instruct on Semiconductor Domain Text\n",
    "==========================================================================\n",
    "\n",
    "This single-file script provides three CLI sub-commands:\n",
    "    1. preprocess - Convert PDFs to clean text, deduplicate, chunk, and build Hugging Face Dataset.\n",
    "    2. train      - Run QLoRA continued pre-training on the processed dataset.\n",
    "    3. evaluate   - Quick perplexity and closed-book QA evaluation.\n",
    "\n",
    "Usage examples\n",
    "--------------\n",
    "# 1. Pre-process raw PDFs (stored in data/raw) and create data/processed/train.jsonl\n",
    "python lora_dapt_pipeline.py preprocess --pdf_dir data/raw --out_dir data/processed --min_tokens 128 --max_tokens 2048 --overlap 256\n",
    "\n",
    "# 2. Train QLoRA adapter (saved to checkpoints/lora)\n",
    "python lora_dapt_pipeline.py train --dataset_path data/processed/train.jsonl --output_dir checkpoints/lora --num_epochs 3 --per_device_train_batch 8\n",
    "\n",
    "# 3. Evaluate perplexity & domain quiz accuracy\n",
    "python lora_dapt_pipeline.py evaluate --adapter_dir checkpoints/lora --eval_questions data/eval/domain_quiz.csv\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# --- PDF → text utility (PyMuPDF)\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    fitz = None  # Will error later if preprocess is used\n",
    "\n",
    "# --- Deduplication\n",
    "try:\n",
    "    from text_dedup.minhash import MinHashDeduper\n",
    "except ImportError:\n",
    "    MinHashDeduper = None\n",
    "\n",
    "# --- Hugging Face & PEFT\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "########################\n",
    "# 1. PRE-PROCESS STAGE #\n",
    "########################\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract raw text from a single PDF page-by-page via PyMuPDF.\"\"\"\n",
    "    if fitz is None:\n",
    "        raise RuntimeError(\"PyMuPDF not installed. pip install pymupdf\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_chunks = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\")\n",
    "        text_chunks.append(page_text)\n",
    "    raw = \"\\n\".join(text_chunks)\n",
    "    return raw\n",
    "\n",
    "SECTION_PATTERNS = [\n",
    "    re.compile(r\"^references$\", re.I),\n",
    "    re.compile(r\"^bibliography$\", re.I),\n",
    "    re.compile(r\"^acknowledg(e)?ments?$\", re.I),\n",
    "]\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove references/acknowledgment sections & excessive blank lines.\"\"\"\n",
    "    lines = [l.strip() for l in text.splitlines()]\n",
    "    cleaned: List[str] = []\n",
    "    skip = False\n",
    "    for ln in lines:\n",
    "        if any(p.match(ln.lower()) for p in SECTION_PATTERNS):\n",
    "            skip = True\n",
    "        if not skip and ln:\n",
    "            cleaned.append(ln)\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "\n",
    "def chunk_text(text: str, tokenizer, max_tokens: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Slice long text into overlapping chunks by token count.\"\"\"\n",
    "    tokens = tokenizer(text)[\"input_ids\"]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk_tokens = tokens[i : i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        if len(chunk_tokens) >= 32:  # minimal meaningful length\n",
    "            chunks.append(chunk_text)\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def preprocess_cmd(args):\n",
    "    pdf_dir = Path(args.pdf_dir)\n",
    "    out_dir = Path(args.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(\"token.txt\", \"r\") as f:\n",
    "        token = f.read().strip()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=token)\n",
    "\n",
    "    raw_records = []\n",
    "    for pdf_path in pdf_dir.rglob(\"*.pdf\"):\n",
    "        raw = extract_text_from_pdf(pdf_path)\n",
    "        cleaned = clean_text(raw)\n",
    "        raw_records.append({\"doc_id\": pdf_path.stem, \"text\": cleaned})\n",
    "        print(f\"[+] Extracted {pdf_path}\")\n",
    "\n",
    "    # Deduplicate\n",
    "    if MinHashDeduper is None:\n",
    "        raise RuntimeError(\"text-dedup not installed. pip install text_dedup\")\n",
    "    texts = [r[\"text\"] for r in raw_records]\n",
    "    deduper = MinHashDeduper(threshold=0.88)\n",
    "    uniques = deduper(texts)\n",
    "    unique_records = [raw_records[i] for i in uniques]\n",
    "    print(f\"[i] Deduplicated: {len(texts)} → {len(unique_records)} docs\")\n",
    "\n",
    "    # Chunking\n",
    "    all_chunks = []\n",
    "    for rec in unique_records:\n",
    "        chunks = chunk_text(rec[\"text\"], tokenizer, args.max_tokens, args.overlap)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\"text\": chunk, \"source\": f\"{rec['doc_id']}§{idx}\"})\n",
    "\n",
    "    # Filter by min_tokens\n",
    "    min_toks = args.min_tokens\n",
    "    def token_len(example):\n",
    "        return len(tokenizer(example[\"text\"])[\"input_ids\"])\n",
    "\n",
    "    all_chunks = [c for c in all_chunks if token_len(c) >= min_toks]\n",
    "    print(f\"[i] Final chunks: {len(all_chunks)}\")\n",
    "\n",
    "    # Write JSONL\n",
    "    jsonl_path = out_dir / \"train.jsonl\"\n",
    "    with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in all_chunks:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"[✓] Saved {jsonl_path}\")\n",
    "\n",
    "########################\n",
    "# 2. TRAINING STAGE    #\n",
    "########################\n",
    "\n",
    "def train_cmd(args):\n",
    "    dataset_path = Path(args.dataset_path)\n",
    "    if not dataset_path.exists():\n",
    "        raise FileNotFoundError(dataset_path)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"json\", data_files=str(dataset_path), split=\"train\")\n",
    "    \n",
    "    with open(\"token.txt\", \"r\") as f:\n",
    "        token = f.read().strip()\n",
    "\n",
    "    # Tokenizer & model\n",
    "    model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 4-bit QLoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "    # PEFT config\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Tokenize dataset lazily\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=args.max_seq_len)\n",
    "\n",
    "    tokenized_ds = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\", \"source\"])\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        per_device_train_batch_size=args.per_device_train_batch,\n",
    "        gradient_accumulation_steps=args.grad_accum,\n",
    "        learning_rate=args.lr,\n",
    "        weight_decay=0.1,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        logging_steps=20,\n",
    "        save_strategy=\"epoch\",\n",
    "        bf16=not args.fp16,\n",
    "        fp16=args.fp16,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    print(\"[✓] Training complete – adapter+tokenizer saved.\")\n",
    "\n",
    "########################\n",
    "# 3. EVALUATION STAGE  #\n",
    "########################\n",
    "\n",
    "def perplexity(eval_texts: List[str], model, tokenizer):\n",
    "    encodings = tokenizer(eval_texts, return_tensors=\"pt\", padding=True)\n",
    "    max_length = encodings.input_ids.shape[1]\n",
    "    stride = 512\n",
    "    ppl_list = []\n",
    "    for i in range(0, max_length, stride):\n",
    "        inputs = {k: v[:, i : i + stride].to(model.device) for k, v in encodings.items()}\n",
    "        with torch.no_grad():\n",
    "            loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
    "        ppl_list.append(torch.exp(loss).item())\n",
    "    return sum(ppl_list) / len(ppl_list)\n",
    "\n",
    "\n",
    "def evaluate_cmd(args):\n",
    "    model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "    # Load LoRA adapter\n",
    "    from peft import PeftModel\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, args.adapter_dir)\n",
    "    model.eval()\n",
    "\n",
    "    # 3-5 random chunks for perplexity\n",
    "    eval_ds = load_dataset(\"json\", data_files=str(args.dataset_path), split=\"train[:1%]\")\n",
    "    sample_texts = [eval_ds[i][\"text\"] for i in range(min(5, len(eval_ds)))]\n",
    "    ppl = perplexity(sample_texts, model, tokenizer)\n",
    "    print(f\"[i] Domain PPL ≈ {ppl:.2f}\")\n",
    "\n",
    "    if args.eval_questions:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(args.eval_questions)\n",
    "        correct = 0\n",
    "        for _, row in df.iterrows():\n",
    "            prompt = textwrap.dedent(\n",
    "                f\"\"\"\\\n",
    "                [INST] {row['question']} [/INST]\n",
    "                \"\"\"\n",
    "            )\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            gen = model.generate(**inputs, max_new_tokens=64)\n",
    "            answer = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "            if re.search(row[\"answer_regex\"], answer, re.I):\n",
    "                correct += 1\n",
    "        print(f\"[i] Quiz accuracy: {correct}/{len(df)} ({100*correct/len(df):.1f}%)\")\n",
    "\n",
    "########################\n",
    "# CLI / Main           #\n",
    "########################\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"LoRA-DAPT pipeline\")\n",
    "    sub = parser.add_subparsers(dest=\"cmd\", required=True)\n",
    "\n",
    "    # preprocess\n",
    "    p_pre = sub.add_parser(\"preprocess\")\n",
    "    p_pre.add_argument(\"--pdf_dir\", default=\"./pdf\", required=True)\n",
    "    p_pre.add_argument(\"--out_dir\", default=\"./result\",  required=True)\n",
    "    p_pre.add_argument(\"--min_tokens\", type=int, default=128)\n",
    "    p_pre.add_argument(\"--max_tokens\", type=int, default=2048)\n",
    "    p_pre.add_argument(\"--overlap\", type=int, default=256)\n",
    "    p_pre.set_defaults(func=preprocess_cmd)\n",
    "\n",
    "    # train\n",
    "    p_train = sub.add_parser(\"train\")\n",
    "    p_train.add_argument(\"--dataset_path\", required=True)\n",
    "    p_train.add_argument(\"--output_dir\", required=True)\n",
    "    p_train.add_argument(\"--num_epochs\", type=int, default=3)\n",
    "    p_train.add_argument(\"--per_device_train_batch\", type=int, default=8)\n",
    "    p_train.add_argument(\"--grad_accum\", type=int, default=4)\n",
    "    p_train.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    p_train.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    p_train.add_argument(\"--lora_r\", type=int, default=64)\n",
    "    p_train.add_argument(\"--lora_alpha\", type=int, default=128)\n",
    "    p_train.add_argument(\"--lora_dropout\", type=float, default=0.05)\n",
    "    p_train.add_argument(\"--max_seq_len\", type=int, default=2048)\n",
    "    p_train.add_argument(\"--fp16\", action=\"store_true\")\n",
    "    p_train.set_defaults(func=train_cmd)\n",
    "\n",
    "    # evaluate\n",
    "    p_eval = sub.add_parser(\"evaluate\")\n",
    "    p_eval.add_argument(\"--adapter_dir\", required=True)\n",
    "    p_eval.add_argument(\"--dataset_path\", required=True)\n",
    "    p_eval.add_argument(\"--eval_questions\", help=\"CSV with columns 'question','answer_regex'\")\n",
    "    p_eval.set_defaults(func=evaluate_cmd)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.func(args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408bb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923695a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
